{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Deep Deterministic Policy Gradients with Instinctive Network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 16:07:25.997511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Lambda, Layer, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "global_seed = 42\n",
    "tf.random.set_seed(global_seed)\n",
    "np.random.seed(global_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Noisy Layer**\n",
    "\n",
    "The NoisyDense layer is a variation of a standard dense layer that incorporates parametric noise into its weights and biases. This noise is learned during training and can help with exploration in reinforcement learning tasks. The layer maintains separate parameters for the mean and standard deviation of weights and biases, allowing it to adapt the amount of noise injected during forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDense(Layer):\n",
    "    \n",
    "    def __init__(self, units, activation=None):\n",
    "        super(NoisyDense, self).__init__()\n",
    "        self.units = units  # Number of output units\n",
    "        self.activation = tf.keras.activations.get(activation)  # Activation function\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize learnable parameters for mean and standard deviation of weights and biases\n",
    "        self.w_mu = self.add_weight(\"w_mu\", shape=(input_shape[-1], self.units))\n",
    "        self.w_sigma = self.add_weight(\"w_sigma\", shape=(input_shape[-1], self.units))\n",
    "        self.b_mu = self.add_weight(\"b_mu\", shape=(self.units,))\n",
    "        self.b_sigma = self.add_weight(\"b_sigma\", shape=(self.units,))\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Generate random noise for weights and biases\n",
    "        w_epsilon = tf.random.normal(self.w_mu.shape)\n",
    "        b_epsilon = tf.random.normal(self.b_mu.shape)\n",
    "        \n",
    "        # Combine mean and noise to create noisy weights and biases\n",
    "        w = self.w_mu + self.w_sigma * w_epsilon\n",
    "        b = self.b_mu + self.b_sigma * b_epsilon\n",
    "        \n",
    "        # Perform the dense layer operation\n",
    "        output = tf.matmul(inputs, w) + b\n",
    "        \n",
    "        # Apply activation function if specified\n",
    "        return self.activation(output) if self.activation else output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Prioritized Experience Replay (PER) Buffer**\n",
    "\n",
    "PER is an improvement over standard experience replay in reinforcement learning. It prioritizes \n",
    "experiences based on their TD-error magnitude, allowing the agent to learn more efficiently from \n",
    "important transitions. This implementation uses proportional prioritization with importance sampling \n",
    "to correct for the bias introduced by non-uniform sampling.\n",
    "\n",
    "#### Key features:\n",
    "- Proportional prioritization: p_i = |δ_i| + ε\n",
    "- Importance sampling weights to correct for bias\n",
    "- Gradual increase of importance sampling (β) over time\n",
    "\n",
    "#### References:\n",
    "[1] Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). Prioritized Experience Replay. \n",
    "    arXiv preprint arXiv:1511.05952."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity, batch_size, alpha=0.6, beta=0.4, beta_increment=1e-3):\n",
    "        # Initialize buffer parameters\n",
    "        self.capacity = capacity  # Maximum number of experiences to store\n",
    "        self.batch_size = batch_size  # Number of experiences to sample in each batch\n",
    "        self.alpha = alpha  # Exponent for prioritization (0 = uniform, 1 = full prioritization)\n",
    "        self.beta = beta  # Initial importance sampling weight\n",
    "        self.beta_increment = beta_increment  # Increment for beta over time\n",
    "        \n",
    "        # Initialize buffer and priorities\n",
    "        self.buffer = np.zeros((capacity, 5, ), dtype=object)  # Buffer to store experiences\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32) + 1e-6  # Priorities for each experience\n",
    "        self.position = 0  # Current position in the buffer\n",
    "        self.size = 0  # Current size of the buffer\n",
    "\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        # Get the maximum priority in the buffer (for new experiences)\n",
    "        max_priority = self.priorities[:self.size].max() if self.size > 0 else 1.0\n",
    "        \n",
    "        # Store the new experience in the buffer\n",
    "        self.buffer[self.position] = [state, action, reward, next_state, done]\n",
    "        \n",
    "        # Assign max priority to the new experience\n",
    "        self.priorities[self.position] = max_priority\n",
    "        \n",
    "        # Update position and size\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        # Check if there are enough samples in the buffer\n",
    "        if self.size < self.batch_size:\n",
    "            return [], [], []\n",
    "\n",
    "        # Calculate sampling probabilities\n",
    "        priorities = self.priorities[:self.size]\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(self.size, self.batch_size, p=probabilities, replace=False)\n",
    "        \n",
    "        # Get the sampled experiences\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        weights = (self.size * probabilities[indices]) ** -self.beta\n",
    "        weights /= np.max(weights)\n",
    "        \n",
    "        # Increase beta for future sampling\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        return experiences, indices, weights\n",
    "\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # Update priorities for the sampled experiences\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority + 1e-6  # Add small constant to avoid zero priority\n",
    "\n",
    "\n",
    "    def isMin(self):\n",
    "        # Check if the buffer has enough samples for a full batch\n",
    "        return self.size >= self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, encoder_dims, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.lr = lr\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.encoder_optimizer = Adam(learning_rate=lr)\n",
    "        self.decoder_optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        x = inputs\n",
    "\n",
    "        for dim in self.encoder_dims:\n",
    "            x = Dense(dim, activation='relu')(x)\n",
    "\n",
    "        mean = Dense(self.latent_dim)(x)\n",
    "        logvar = Dense(self.latent_dim)(x)\n",
    "        z = Lambda(lambda x: self.reparameterize(x[0], x[1]))([mean, logvar])\n",
    "        return Model(inputs, [z, mean, logvar])\n",
    "\n",
    "\n",
    "    def build_decoder(self):\n",
    "        inputs = Input(shape=(self.latent_dim,))\n",
    "        x = inputs\n",
    "\n",
    "        for dim in reversed(self.encoder_dims):\n",
    "            x = Dense(dim, activation='relu')(x)\n",
    "\n",
    "        outputs = Dense(self.input_dim)(x)\n",
    "        return Model(inputs, outputs)\n",
    "    \n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z, mean, logvar = self.encode(x)\n",
    "            x_reconstructed = self.decode(z)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.square(x - x_reconstructed))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        gradients = tape.gradient(total_loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
    "        self.encoder_optimizer.apply_gradients(zip(gradients[:len(self.encoder.trainable_variables)], self.encoder.trainable_variables))\n",
    "        self.decoder_optimizer.apply_gradients(zip(gradients[len(self.encoder.trainable_variables):], self.decoder.trainable_variables))\n",
    "\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Multi Head Actor**\n",
    "\n",
    "Multi-Head Actor for Deep Deterministic Policy Gradient (DDPG) algorithm.\n",
    "\n",
    "This class implements a multi-head actor network that outputs multiple action\n",
    "predictions for each state. It uses a shared network followed by multiple\n",
    "output heads, one for each kernel. The final output is a combination of these\n",
    "heads, processed through a noisy dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Actor(object):\n",
    "    def __init__(self, inp_dim, out_dim, act_range, lr, tau):\n",
    "        self.inp_dim = inp_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.act_range = act_range\n",
    "        self.tau = tau\n",
    "        self.noise_std = 0.1\n",
    "        self.noise_clip = 0.5\n",
    "\n",
    "        self.model = self.buildNetwork()\n",
    "        self.target_model = self.buildNetwork()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=lr,\n",
    "            decay_steps=100000,\n",
    "            decay_rate=0.99\n",
    "        )\n",
    "        self.optimizer = Adam(learning_rate=self.lr_schedule)\n",
    "\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        inp = Input(shape=(self.inp_dim,))\n",
    "        x = Dense(256, activation='relu')(inp)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        output = Dense(self.out_dim, activation='tanh')(x)\n",
    "        output = Lambda(lambda x: x * self.act_range)(output)\n",
    "        return Model(inputs=inp, outputs=output)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, states):\n",
    "        return self.model(states)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def target_predict(self, states):\n",
    "        return self.target_model(states)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def transferWeights(self):\n",
    "        for a, b in zip(self.target_model.variables, self.model.variables):\n",
    "            a.assign(self.tau * b + (1 - self.tau) * a)\n",
    "            \n",
    "\n",
    "    def add_noise(self, actions):\n",
    "        noise = np.random.normal(0, self.noise_std, size=actions.shape)\n",
    "        noise = np.clip(noise, -self.noise_clip, self.noise_clip)\n",
    "        return np.clip(actions + noise, -self.act_range, self.act_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Critic(object):\n",
    "    def __init__(self, state_dim, action_dim, lr, tau):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.tau = tau\n",
    "\n",
    "        self.model1 = self.buildNetwork()\n",
    "        self.model2 = self.buildNetwork()\n",
    "        self.target_model1 = self.buildNetwork()\n",
    "        self.target_model2 = self.buildNetwork()\n",
    "        \n",
    "        self.target_model1.set_weights(self.model1.get_weights())\n",
    "        self.target_model2.set_weights(self.model2.get_weights())\n",
    "\n",
    "        self.lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=lr,\n",
    "            decay_steps=100000,\n",
    "            decay_rate=0.99\n",
    "        )\n",
    "        self.optimizer1 = Adam(learning_rate=self.lr_schedule)\n",
    "        self.optimizer2 = Adam(learning_rate=self.lr_schedule)\n",
    "        \n",
    "\n",
    "    def buildNetwork(self):\n",
    "        state_input = Input(shape=(self.state_dim,))\n",
    "        action_input = Input(shape=(self.action_dim,))\n",
    "        x = Concatenate()([state_input, action_input])\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        output = Dense(1)(x)\n",
    "        return Model(inputs=[state_input, action_input], outputs=output)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, states, actions):\n",
    "        return self.model1([states, actions]), self.model2([states, actions])\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def target_predict(self, states, actions):\n",
    "        return self.target_model1([states, actions]), self.target_model2([states, actions])\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def transferWeights(self):\n",
    "        for a, b in zip(self.target_model1.variables, self.model1.variables):\n",
    "            a.assign(self.tau * b + (1 - self.tau) * a)\n",
    "        for a, b in zip(self.target_model2.variables, self.model2.variables):\n",
    "            a.assign(self.tau * b + (1 - self.tau) * a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **DDPG Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Agent(object):\n",
    "    def __init__(\n",
    "        self, state_dim, action_dim, action_min, action_max, \n",
    "        memory_size, batch_size, gamma, a_lr, c_lr, tau, max_steps, \n",
    "        env_name, policy_noise=0.2, noise_clip=0.5, policy_freq=2\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.max_steps = max_steps\n",
    "        self.env_name = env_name\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "\n",
    "        self.memory = PrioritizedReplayBuffer(memory_size, batch_size)\n",
    "\n",
    "        self.actor = TD3Actor(\n",
    "            inp_dim=self.state_dim, \n",
    "            out_dim=self.action_dim,\n",
    "            act_range=self.action_max, \n",
    "            lr=a_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "        self.critic = TD3Critic(\n",
    "            state_dim=self.state_dim, \n",
    "            action_dim=self.action_dim,\n",
    "            lr=c_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "\n",
    "    def create_plot(self):\n",
    "        # Create a figure for SOM activation visualization\n",
    "        self.fig = plt.figure()\n",
    "\n",
    "        self.returns = self.fig.add_subplot(211)\n",
    "        self.returns.title.set_text('Retruns')\n",
    "\n",
    "        self.n_steps = self.fig.add_subplot(212)\n",
    "        self.n_steps.title.set_text('N Steps')\n",
    "\n",
    "        self.fig.show()\n",
    "        return\n",
    "\n",
    "\n",
    "    def update_plots(self, returns, n_steps):\n",
    "        # Update the SOM activation plot\n",
    "        self.returns.plot(np.arange(len(returns)), returns)\n",
    "\n",
    "        self.n_steps.plot(np.arange(len(returns)), n_steps)\n",
    "\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "        return\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def policy(self, state):\n",
    "        action = self.actor.predict(tf.expand_dims(state, 0))\n",
    "        return tf.clip_by_value(action[0], self.action_min, self.action_max)\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "        self.replay_memory()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def update_nets(self, weights, states, actions, rewards, next_states, dones):\n",
    "        weights = tf.cast(weights, dtype=tf.float32)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = tf.random.normal(tf.shape(actions), stddev=self.policy_noise)\n",
    "            noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_actions = self.actor.target_predict(next_states) + noise\n",
    "            next_actions = tf.clip_by_value(next_actions, self.action_min, self.action_max)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic.target_predict(next_states, next_actions)\n",
    "            target_Q = tf.minimum(target_Q1, target_Q2)\n",
    "            target_Q = rewards + (1 - dones) * self.gamma * target_Q\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic.predict(states, actions)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = tf.reduce_mean(tf.square(target_Q - current_Q1)) + tf.reduce_mean(tf.square(target_Q - current_Q2))\n",
    "\n",
    "        # Optimize the critic\n",
    "        critic_grad1 = tape.gradient(critic_loss, self.critic.model1.trainable_variables)\n",
    "        critic_grad2 = tape.gradient(critic_loss, self.critic.model2.trainable_variables)\n",
    "        self.critic.optimizer1.apply_gradients(zip(critic_grad1, self.critic.model1.trainable_variables))\n",
    "        self.critic.optimizer2.apply_gradients(zip(critic_grad2, self.critic.model2.trainable_variables))\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "                actor_loss = -tf.reduce_mean(self.critic.model1([states, self.actor.predict(states)]))\n",
    "\n",
    "            actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "            self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "\n",
    "            # Update the frozen target models\n",
    "            self.actor.transferWeights()\n",
    "            self.critic.transferWeights()\n",
    "\n",
    "        return target_Q, tf.minimum(current_Q1, current_Q2)\n",
    "\n",
    "\n",
    "\n",
    "    def replay_memory(self):\n",
    "        if not self.memory.isMin(): return  # Not enough samples in the buffer\n",
    "        \n",
    "        # Sample from replay buffer and perform learning update\n",
    "        experiences, indices, weights = self.memory.sample()\n",
    "        \n",
    "        states = tf.convert_to_tensor([exp[0] for exp in experiences], dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor([exp[1] for exp in experiences], dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor([exp[2] for exp in experiences], dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor([exp[3] for exp in experiences], dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor([exp[4] for exp in experiences], dtype=tf.float32)\n",
    "\n",
    "        #self.actor.vae.train_step(tf.concat([states, next_states], axis=0))\n",
    "        y, critic_value = self.update_nets(weights, states, actions, rewards, next_states, dones)\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Update priorities in the replay buffer\n",
    "        td_errors = tf.abs(y - critic_value)\n",
    "        self.memory.update_priorities(indices, td_errors.numpy().flatten())\n",
    "        return\n",
    "\n",
    "\n",
    "    def act(self):\n",
    "        # Perform a single episode in the environment using the current policy\n",
    "        env2 = gym.make(self.env_name, hardcore=True, render_mode='human')\n",
    "        observation, _ = env2.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            env2.render()\n",
    "            action = self.policy(observation)\n",
    "            new_observation, _, done, _, _ = env2.step(action.numpy())\n",
    "            observation = new_observation\n",
    "            step += 1\n",
    "            done = done or (step > self.max_steps)\n",
    "        \n",
    "        env2.close()\n",
    "        return\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self, env, num_episodes, verbose, verbose_num, end_on_complete, \n",
    "        complete_num, complete_value, act_after_batch\n",
    "    ):\n",
    "        # Main training loop\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "\n",
    "        print(\"BEGIN\\n\")\n",
    "        complete = 0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            steps = 0\n",
    "            observation, _ = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action = self.policy(observation)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"\\r                                                          \", end=\"\")\n",
    "                    print(f\"\\rEpisode: {str(episode+1)} \\tStep: {str(steps)} \\tReward: {str(score)}\", end=\"\")\n",
    "                \n",
    "                new_observation, reward, done, _, _ = env.step(action.numpy())\n",
    "                \n",
    "                if steps > self.max_steps:\n",
    "                    reward = -100\n",
    "                    done = True\n",
    "\n",
    "                self.learn(observation, action.numpy(), reward, new_observation, done)\n",
    "                observation = new_observation\n",
    "                score += reward\n",
    "                steps += 1\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            self.update_plots(scores_history, steps_history)\n",
    "            \n",
    "            if score >= complete_value:\n",
    "                complete += 1\n",
    "                if end_on_complete and complete >= complete_num: break\n",
    "            \n",
    "            if (episode+1) % verbose_num == 0:\n",
    "                print(\"\\r                                                 \", end=\"\")\n",
    "                print(f'''\\rEpisodes: {episode+1}/{num_episodes}\\n\\tTotal reward: {np.mean(scores_history[-verbose_num:])} +- {np.std(scores_history[-verbose_num:])}\\n\\tNum. steps: {np.mean(steps_history[-verbose_num:])} +- {np.std(steps_history[-verbose_num:])}\\n\\tCompleted: {complete}\\n--------------------------''')\n",
    "                \n",
    "                if act_after_batch: self.act()\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        \n",
    "        return scores_history, steps_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"BipedalWalker-v3\"\n",
    "env = gym.make(name, hardcore=True)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "\n",
    "memory_size = 1000000\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "a_lr = 1e-4\n",
    "c_lr = 1e-3\n",
    "tau = 3e-3\n",
    "max_steps = 1000\n",
    "n_kernels = 3\n",
    "\n",
    "agent = TD3Agent(\n",
    "    state_dim, action_dim, action_min, action_max, \n",
    "    memory_size, batch_size, gamma, a_lr, c_lr, tau, \n",
    "    max_steps, name, n_kernels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episodes: 5/3000                                          \n",
      "\tTotal reward: -110.86066195288781 +- 12.343957072047168\n",
      "\tNum. steps: 98.6 +- 45.06262309275837\n",
      "\tCompleted: 0\n",
      "--------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Binding inputs to tf.function `policy` failed due to `got an unexpected keyword argument 'train'`.Received args: (array([ 2.7474314e-03, -1.2279355e-05,  9.5520884e-04, -1.5999917e-02,\n        9.1980457e-02, -1.2605514e-03,  8.6025876e-01,  2.3344464e-03,\n        1.0000000e+00,  3.2387462e-02, -1.2604642e-03,  8.5380626e-01,\n        8.9147344e-04,  1.0000000e+00,  4.4081402e-01,  4.4582012e-01,\n        4.6142277e-01,  4.8955020e-01,  5.3410280e-01,  6.0246104e-01,\n        7.0914888e-01,  8.8593185e-01,  1.0000000e+00,  1.0000000e+00],\n      dtype=float32),) and kwargs: {'train': False} for signature: (state).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m complete_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      7\u001b[0m act_after_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_on_complete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplete_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplete_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_after_batch\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 208\u001b[0m, in \u001b[0;36mTD3Agent.train\u001b[0;34m(self, env, num_episodes, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m                                                 \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mEpisodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTotal reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(scores_history[\u001b[38;5;241m-\u001b[39mverbose_num:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(scores_history[\u001b[38;5;241m-\u001b[39mverbose_num:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mNum. steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(steps_history[\u001b[38;5;241m-\u001b[39mverbose_num:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(steps_history[\u001b[38;5;241m-\u001b[39mverbose_num:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mCompleted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomplete\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m act_after_batch: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m         complete \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFINISHED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 152\u001b[0m, in \u001b[0;36mTD3Agent.act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    151\u001b[0m     env2\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 152\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     new_observation, _, done, _, _ \u001b[38;5;241m=\u001b[39m env2\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    154\u001b[0m     observation \u001b[38;5;241m=\u001b[39m new_observation\n",
      "File \u001b[0;32m~/.application-data/miniconda3/envs/rl-tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.application-data/miniconda3/envs/rl-tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:428\u001b[0m, in \u001b[0;36mFunctionSpec.bind_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind_with_defaults(\n\u001b[1;32m    426\u001b[0m       args, sanitized_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_values)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 428\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\u001b[38;5;241m.\u001b[39margs, bound_arguments\u001b[38;5;241m.\u001b[39mkwargs\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function `policy` failed due to `got an unexpected keyword argument 'train'`.Received args: (array([ 2.7474314e-03, -1.2279355e-05,  9.5520884e-04, -1.5999917e-02,\n        9.1980457e-02, -1.2605514e-03,  8.6025876e-01,  2.3344464e-03,\n        1.0000000e+00,  3.2387462e-02, -1.2604642e-03,  8.5380626e-01,\n        8.9147344e-04,  1.0000000e+00,  4.4081402e-01,  4.4582012e-01,\n        4.6142277e-01,  4.8955020e-01,  5.3410280e-01,  6.0246104e-01,\n        7.0914888e-01,  8.8593185e-01,  1.0000000e+00,  1.0000000e+00],\n      dtype=float32),) and kwargs: {'train': False} for signature: (state)."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "verbose = True\n",
    "verbose_num = 5\n",
    "end_on_complete = True\n",
    "complete_num = 2\n",
    "complete_value = 300\n",
    "act_after_batch = True\n",
    "\n",
    "agent.train(\n",
    "    env, num_episodes, verbose, \n",
    "    verbose_num, end_on_complete,  \n",
    "    complete_num, complete_value, \n",
    "    act_after_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
