{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Deep Deterministic Policy Gradients with Instinctive Network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import gym, time, math\n",
    "import tensorflow as tf, matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "from tensorflow import random_uniform_initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda, BatchNormalization \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from typing import Tuple, List\n",
    "from minisom import MiniSom\n",
    "\n",
    "global_seed = 42\n",
    "tf.random.set_seed(global_seed)\n",
    "np.random.seed(global_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **OU Action Noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.1, x0=None):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "a = np.zeros(15)\n",
    "b = OUActionNoise(a)\n",
    "a += b()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, minibatch_size = None):\n",
    "        '''\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        '''\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "        self.max_size = size\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def append(self, state, action, reward, next_state, context, next_context, done):\n",
    "        '''\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            done (boolen): True if the next state is a terminal state and False otherwise.\n",
    "            Is transformed to integer so tha True = 1, False = 0\n",
    "            next_state (Numpy array): The next state.           \n",
    "        '''\n",
    "        if self.size() == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, next_state, context, next_context, int(done)])\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        '''\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def size(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def isMin(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Boolean indicating if the memory have the minimum number of elements or not\n",
    "        '''\n",
    "        return (self.size() >= self.minibatch_size)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def empties(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def getEpisode(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            List with all the elements in the buffer\n",
    "        '''\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Instinctive Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstinctiveWeights:\n",
    "    def __init__(self, som_shape: Tuple[int, int]):\n",
    "        self.som_shape = som_shape\n",
    "        self.num_neurons = np.prod(self.som_shape)\n",
    "        self.weights = np.zeros(int((self.num_neurons*(self.num_neurons+1))/2))\n",
    "        self.activations = np.zeros(self.som_shape)\n",
    "\n",
    "\n",
    "    def __neuron_to_position(self, neuron: Tuple[int, int]) -> int:\n",
    "        i, j = neuron\n",
    "        return int((i*self.som_shape[1]) + j)\n",
    "\n",
    "\n",
    "    def __positions_to_index(self, position: Tuple[int, int]) -> int:\n",
    "        i, j = position\n",
    "        return int(((i*(i+1))/2) + j)\n",
    "\n",
    "\n",
    "    def __get_tril_positions(self, n1: Tuple[int, int], n2: Tuple[int, int]) -> Tuple[int, int]:\n",
    "        n1_pos_aux = self.__neuron_to_position(n1)\n",
    "        n2_pos_aux = self.__neuron_to_position(n2)\n",
    "\n",
    "        n1_pos = max(n1_pos_aux, n2_pos_aux)\n",
    "        n2_pos = min(n1_pos_aux, n2_pos_aux)\n",
    "\n",
    "        return n1_pos, n2_pos\n",
    "\n",
    "\n",
    "    def reinforce_connection(self, n1: Tuple[int, int], n2: Tuple[int, int]) -> None:\n",
    "        weight_position = self.__get_tril_positions(n1, n2)\n",
    "        array_index = self.__positions_to_index(weight_position)\n",
    "        value = self.weights[array_index]\n",
    "\n",
    "        value += 0.1\n",
    "        value = np.clip(value, 0, 1)\n",
    "\n",
    "        self.weights[array_index] = value\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        self.weights *= 0.995\n",
    "\n",
    "    \n",
    "    def get_weight_matrix(self):\n",
    "        full_matrix = np.zeros((self.num_neurons, self.num_neurons))\n",
    "\n",
    "        # Fill the lower triangular part of the matrix\n",
    "        index = 0\n",
    "        for i in range(self.num_neurons):\n",
    "            for j in range(i + 1):\n",
    "                full_matrix[i, j] = self.weights[index]\n",
    "                index += 1\n",
    "\n",
    "        # Mirror the lower triangular part to the upper triangular part\n",
    "        full_matrix = full_matrix + full_matrix.T - np.diag(np.diag(full_matrix))\n",
    "        \n",
    "        return full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Instinctive Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstinctiveLayer:\n",
    "    def __init__(\n",
    "        self, som_shape: Tuple[int, int], func_x_max: int, func_x_drop: float, \n",
    "        func_y_max: float, func_y_min: float, act_threshold: float,\n",
    "    ):\n",
    "        self.som_shape = som_shape\n",
    "        self.act_threshold = act_threshold\n",
    "        self.func_x_max = func_x_max\n",
    "        self.func_x_drop = func_x_drop\n",
    "        self.func_y_max = func_y_max\n",
    "        self.func_y_min = func_y_min\n",
    "\n",
    "        self.charges = np.zeros(self.som_shape) + self.func_x_max/100\n",
    "\n",
    "\n",
    "    def get_act_array(self):\n",
    "        return np.reshape(self.get_activations(), -1)\n",
    "\n",
    "\n",
    "    def get_active(self) -> List[Tuple[int, int]]:\n",
    "        act = self.get_activations()\n",
    "        active = np.where(act == 1)\n",
    "        return [(i, j) for i, j in zip(*active)]\n",
    "\n",
    "\n",
    "    def step(self, weights, som_act) -> None:        \n",
    "        charge_delta = np.reshape(np.dot((self.charges * self.get_activations()).reshape((1, -1)), weights), self.charges.shape)\n",
    "        charge_delta = np.clip(charge_delta, 0, self.func_x_max/50)\n",
    "        \n",
    "        som_act *= self.func_x_max/50\n",
    "\n",
    "        self.charges += (charge_delta + som_act)\n",
    "\n",
    "        self.charges[self.charges > self.func_x_drop] += self.func_x_max/100\n",
    "        self.charges[self.charges <= self.func_x_drop] -= self.func_x_max/100\n",
    "\n",
    "        self.charges[self.charges > self.func_x_max] = 0\n",
    "        self.charges[self.charges < 0] = 0\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def __apply_activation_function(self, charges):\n",
    "    # Define the piecewise function\n",
    "        func = np.vectorize(lambda charges_data: np.piecewise(\n",
    "            charges_data,\n",
    "            [\n",
    "                charges_data <= self.func_x_drop, \n",
    "                self.func_x_drop < charges_data <= self.func_x_max,\n",
    "            ],\n",
    "            [\n",
    "                lambda x: (self.func_y_max / self.func_x_drop**2) * x**2,  # Second degree growth\n",
    "                lambda x: self.func_y_min + (0 - self.func_y_min) / (self.func_x_max - self.func_x_drop) * (x - self.func_x_drop) # Linear growth\n",
    "            ]\n",
    "        ))\n",
    "        return func(charges)\n",
    "    \n",
    "\n",
    "    def reset_charges(self):\n",
    "        self.charges = np.zeros(self.som_shape) + self.func_x_max/100\n",
    "    \n",
    "    \n",
    "    def get_activations(self):\n",
    "        activations = self.__apply_activation_function(self.charges)\n",
    "        activations = np.clip(activations, 0, self.act_threshold)\n",
    "        activations = activations/self.act_threshold\n",
    "        activations[self.charges > self.func_x_drop] = self.func_x_max/100\n",
    "        return activations\n",
    "    \n",
    "    \n",
    "    def plot_act_func(self):\n",
    "        # Generate x values\n",
    "        x = np.linspace(0, self.func_x_max, 1000)\n",
    "        y = self.__apply_activation_function(x)\n",
    "\n",
    "        # Plot the function\n",
    "        plt.plot(x, y, label=\"Piecewise Function with Repetition\")\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.title('Piecewise Function: Exponential Growth, Peak, Drop, Linear Growth, and Repeat')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.axhline(y=self.func_y_max, color='gray', linestyle='--')\n",
    "        plt.axhline(y=self.func_y_min, color='gray', linestyle='--')\n",
    "        plt.axvline(x=self.func_x_drop, color='gray', linestyle='--')\n",
    "        plt.axvline(x=self.func_x_max, color='gray', linestyle='--')\n",
    "        plt.axhline(y = 0, color = 'r', linestyle = '-') \n",
    "        plt.axhline(y = self.act_threshold, color = 'r', linestyle = '-') \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "som_shape = (4, 4)\n",
    "func_x_max = 50\n",
    "func_x_drop = 30 \n",
    "func_y_max = 10 \n",
    "func_y_min = -7 \n",
    "act_threshold = 5\n",
    "\n",
    "inst_w = InstinctiveWeights(som_shape)\n",
    "inst_l= InstinctiveLayer(\n",
    "    som_shape = som_shape,\n",
    "    func_x_max = func_x_max, \n",
    "    func_x_drop = func_x_drop, \n",
    "    func_y_max = func_y_max, \n",
    "    func_y_min = func_y_min, \n",
    "    act_threshold = act_threshold,\n",
    ")\n",
    "\n",
    "display(inst_l.plot_act_func())\n",
    "\n",
    "def get_act():\n",
    "    inst_l.step(inst_w.get_weight_matrix(), np.random.uniform(low=0, high=1, size=som_shape))\n",
    "    return inst_l.get_activations(), inst_l.charges.copy()\n",
    "\n",
    "_ = [inst_w.reinforce_connection(**connection) \n",
    "    for connection in [\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 3)},\n",
    "        {'n1': (0, 3), 'n2': (0, 0)},\n",
    "        {'n1': (0, 0), 'n2': (0, 0)},\n",
    "        {'n1': (0, 0), 'n2': (0, 0)},\n",
    "        {'n1': (0, 0), 'n2': (0, 1)},\n",
    "        {'n1': (0, 1), 'n2': (1, 0)},\n",
    "    ]]\n",
    "acts = [get_act() for _ in range(10)]\n",
    "\n",
    "_ = [inst_w.reinforce_connection(**connection) \n",
    "    for connection in [\n",
    "        {'n1': (0, 1), 'n2': (1, 0)},\n",
    "        {'n1': (0, 1), 'n2': (1, 0)},\n",
    "        {'n1': (1, 0), 'n2': (2, 2)},\n",
    "        {'n1': (2, 2), 'n2': (2, 2)},\n",
    "        {'n1': (2, 2), 'n2': (0, 3)},\n",
    "        {'n1': (2, 2), 'n2': (0, 3)},\n",
    "        {'n1': (2, 2), 'n2': (0, 3)},\n",
    "    ]]\n",
    "acts.extend([get_act() for _ in range(10)])\n",
    "\n",
    "arrays = [act[0] for act in acts]\n",
    "\n",
    "# Determine the number of subplots\n",
    "num_arrays = len(arrays)\n",
    "n = arrays[0].shape[0]\n",
    "\n",
    "# Determine the grid size\n",
    "cols = int(np.ceil(np.sqrt(num_arrays)))\n",
    "rows = int(np.ceil(num_arrays / cols))\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(8, 5))\n",
    "\n",
    "# Plot each array in its subplot\n",
    "for i, array in enumerate(arrays):\n",
    "    ax = axes.flat[i]\n",
    "    cax = ax.matshow(array, cmap='viridis', vmin=0, vmax=1)\n",
    "    fig.colorbar(cax, ax=ax)\n",
    "    ax.set_title(f'Array {i + 1}')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes.flat)):\n",
    "    fig.delaxes(axes.flat[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Instinctive Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstinctiveNetwork:\n",
    "\n",
    "    def __init__(self, som_dims: Tuple[int, int], input_dim: int, som_kwargs: dict, inst_net_kwargs: dict):\n",
    "        self.som_dims = som_dims\n",
    "        self.input_dim = input_dim\n",
    "        self.som = MiniSom(*self.som_dims, self.input_dim, **som_kwargs)\n",
    "        self.inner_weights = InstinctiveWeights(self.som_dims)\n",
    "        self.inner_layer = InstinctiveLayer(som_shape = self.som_dims, **inst_net_kwargs)\n",
    "        self.last_winner = None\n",
    "\n",
    "\n",
    "    def train_som(self, data):\n",
    "        self.som.train(data, 5, verbose=False)\n",
    "\n",
    "\n",
    "    def reinforce_connection(self, data):\n",
    "        som_winner = self.som.winner(data)\n",
    "        if self.last_winner is not None:\n",
    "            self.inner_weights.reinforce_connection(self.last_winner, som_winner)\n",
    "        self.last_winner = som_winner\n",
    "        return\n",
    "    \n",
    "\n",
    "    def reset_charges(self):\n",
    "        self.inner_layer.reset_charges()\n",
    "        self.last_winner = None\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_output(self, data, reinforce=True):\n",
    "        if reinforce:\n",
    "            self.inner_weights.step()\n",
    "            self.reinforce_connection(data)\n",
    "\n",
    "        som_act = self.som.activate([data])\n",
    "        som_act = 1 - ((som_act-np.amin(som_act))/(np.amax(som_act)-np.amin(som_act)))\n",
    "        \n",
    "        som_act_dist = som_act.copy()\n",
    "        som_act_dist[som_act_dist < 0.9] = 0\n",
    "\n",
    "        weights = self.inner_weights.get_weight_matrix()\n",
    "        self.inner_layer.step(weights, som_act_dist)\n",
    "\n",
    "        # active = self.inner_layer.get_active()\n",
    "        # active = np.array([self.som.get_weights()[x, y] for x, y in active])\n",
    "        # if not len(active):\n",
    "        #     return np.zeros(self.input_dim), act\n",
    "        # active = np.mean(active, axis=0)\n",
    "\n",
    "        active = self.inner_layer.get_act_array()\n",
    "        return active, som_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "inst_net = InstinctiveNetwork(\n",
    "    som_dims = (5, 5), \n",
    "    input_dim = 16,\n",
    "    som_kwargs = {\n",
    "        'sigma': 3,\n",
    "        'learning_rate': 0.05,\n",
    "    }, \n",
    "    inst_net_kwargs = {\n",
    "        'func_x_max': 50,\n",
    "        'func_x_drop': 30,\n",
    "        'func_y_max': 10,\n",
    "        'func_y_min': -7,\n",
    "        'act_threshold': 5,\n",
    "    }\n",
    ")\n",
    "\n",
    "data = np.random.rand(100, 16)\n",
    "inst_net.train_som(data)\n",
    "out = []\n",
    "\n",
    "for i in data:\n",
    "    inst_net.reinforce_connection(i)\n",
    "    out.append(inst_net.get_output(i))\n",
    "\n",
    "np.shape(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adaptar actor e critic para receber inputs da InstinctiveNetwork e trinamentos da instinctive:\n",
    "    # * treina SOM junto com redes -> pega todos os estados/ações da batch e treina\n",
    "    # * treinar inst_layer em relatime, a medida que for gerando um novo estado treina ao gerar a saida para ele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Actor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, s_inp_dim, s_fc1_dim, con_inp_dim, con_fc1_dim, fc2_dim, fc3_dim, out_dim, act_range, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.s_inp_dim = s_inp_dim\n",
    "        self.s_fc1_dim = s_fc1_dim\n",
    "        self.con_inp_dim = con_inp_dim\n",
    "        self.con_fc1_dim = con_fc1_dim\n",
    "        self.fc2_dim = fc2_dim\n",
    "        self.fc3_dim = fc3_dim\n",
    "        self.out_dim = out_dim\n",
    "        #Range of the action space\n",
    "        self.act_range = act_range\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        #Generates the optimization function\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "        #Generates the actor model\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generates the actor target model\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        inp = Input(shape=(self.s_inp_dim,))\n",
    "        f1 = 1 / np.sqrt(self.s_fc1_dim)\n",
    "        fc1 = Dense(self.s_fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(inp)\n",
    "        norm1 = BatchNormalization(dtype='float64')(fc1)\n",
    "\n",
    "        inp_con = Input(shape=(self.con_inp_dim,))\n",
    "        f1 = 1 / np.sqrt(self.con_fc1_dim)\n",
    "        fc1_con = Dense(self.con_fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(inp_con)\n",
    "        norm1_con = BatchNormalization(dtype='float64')(fc1_con)\n",
    "        \n",
    "        c_inp = Concatenate(dtype='float64')([norm1, norm1_con])\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.fc2_dim)\n",
    "        fc2 = Dense(self.fc2_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2), dtype='float64')(c_inp)\n",
    "        norm2 = BatchNormalization(dtype='float64')(fc2)\n",
    "\n",
    "        f3 = 1 / np.sqrt(self.fc3_dim)\n",
    "        fc3 = Dense(self.fc3_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(norm2)\n",
    "        norm3 = BatchNormalization(dtype='float64')(fc3)\n",
    "        \n",
    "        f3 = 0.003\n",
    "        out = Dense(self.out_dim, activation='tanh', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(norm3)\n",
    "        lamb = Lambda(lambda i: i * self.act_range, dtype='float64')(out)\n",
    "        \n",
    "        return Model(inputs=[inp, inp_con], outputs=[lamb])\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, contexts):\n",
    "        return self.model([states, contexts], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, contexts):\n",
    "        return self.target_model([states, contexts], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, state_inp_dim, state_fc1_dim, action_inp_dim, action_fc1_dim, conc_fc1_dim, conc_fc2_dim, out_dim, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.state_inp_dim = state_inp_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.action_inp_dim = action_inp_dim\n",
    "        self.action_fc1_dim = action_fc1_dim\n",
    "        self.conc_fc2_dim = conc_fc2_dim\n",
    "        self.conc_fc1_dim = conc_fc1_dim\n",
    "        self.out_dim = out_dim\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        #Define the critic optimizer\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Generate the critic network\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generate the critic target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        #State input network ---------\n",
    "        s_inp = Input(shape=(self.state_inp_dim, ))\n",
    "        f1 = 1 / np.sqrt(self.state_fc1_dim)\n",
    "        s_fc1 = Dense(\n",
    "            self.state_fc1_dim, activation='relu', \n",
    "            kernel_initializer=random_uniform_initializer(-f1, f1), \n",
    "            bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64'\n",
    "        )(s_inp)\n",
    "        s_norm1 = BatchNormalization(dtype='float64')(s_fc1)\n",
    "        \n",
    "        #Action input network ---------\n",
    "        a_inp = Input(shape=(self.action_inp_dim, ))\n",
    "        f1 = 1 / np.sqrt(self.action_fc1_dim)\n",
    "        a_fc1 = Dense(\n",
    "            self.action_fc1_dim, activation='relu', \n",
    "            kernel_initializer=random_uniform_initializer(-f1, f1), \n",
    "            bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64'\n",
    "        )(a_inp)\n",
    "        a_norm1 = BatchNormalization(dtype='float64')(a_fc1)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = Concatenate(dtype='float64')([s_norm1, a_norm1])\n",
    "        \n",
    "        #Creates the output network\n",
    "        f1 = 1 / np.sqrt(self.conc_fc1_dim)\n",
    "        c_fc1 = Dense(\n",
    "            self.conc_fc1_dim, activation='relu', \n",
    "            kernel_initializer=random_uniform_initializer(-f1, f1), \n",
    "            bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64'\n",
    "        )(c_inp)\n",
    "        c_norm1 = BatchNormalization(dtype='float64')(c_fc1)\n",
    "\n",
    "        f2 = 1 / np.sqrt(self.conc_fc2_dim)\n",
    "        c_fc2 = Dense(\n",
    "            self.conc_fc2_dim, activation='relu', \n",
    "            kernel_initializer=random_uniform_initializer(-f2, f2), \n",
    "            bias_initializer=random_uniform_initializer(-f2, f2), dtype='float64'\n",
    "        )(c_norm1)\n",
    "        c_norm2 = BatchNormalization(dtype='float64')(c_fc2)\n",
    "        \n",
    "        f3 = 0.003\n",
    "        out = Dense(\n",
    "            self.out_dim, activation='linear', \n",
    "            kernel_initializer=random_uniform_initializer(-f3, f3), \n",
    "            bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64'\n",
    "        )(c_norm2)\n",
    "        \n",
    "        model = Model(inputs=[s_inp, a_inp], outputs=[out])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, actions):\n",
    "        return self.model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, actions):\n",
    "        return self.target_model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **DDPG Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent(object):\n",
    "    def __init__(\n",
    "        self, state_dim, action_dim, action_min, action_max, \n",
    "        memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, \n",
    "        epsilon_decay, epsilon_min, max_steps, env_name\n",
    "    ):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.a_lr = a_lr\n",
    "        self.c_lr = c_lr\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.max_steps = max_steps\n",
    "        self.env_name = env_name\n",
    "\n",
    "        #Creates the Replay Buffer\n",
    "        self.memory = ReplayBuffer(self.memory_size, self.batch_size)\n",
    "\n",
    "        # creates instinctive network\n",
    "        som_dim = (15, 15)\n",
    "        self.inst_net = InstinctiveNetwork(\n",
    "            som_dims = som_dim, \n",
    "            input_dim = self.state_dim,\n",
    "            som_kwargs = {\n",
    "                'sigma': 1,\n",
    "                'learning_rate': 0.5,\n",
    "                'decay_function': lambda x, y, z: x,\n",
    "                'neighborhood_function': 'bubble'\n",
    "            }, \n",
    "            inst_net_kwargs = {\n",
    "                'func_x_max': 50,\n",
    "                'func_x_drop': 35,\n",
    "                'func_y_max': 10,\n",
    "                'func_y_min': -7,\n",
    "                'act_threshold': 4,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Creates the actor\n",
    "        self.actor = Actor(\n",
    "            s_inp_dim=self.state_dim, \n",
    "            s_fc1_dim=128,\n",
    "            con_inp_dim=np.prod(som_dim), \n",
    "            con_fc1_dim=1024,\n",
    "            fc2_dim=256, \n",
    "            fc3_dim=64,\n",
    "            out_dim=self.action_dim, \n",
    "            act_range=self.action_max, \n",
    "            lr=self.a_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "        #Creates the critic\n",
    "        self.critic = Critic(\n",
    "            state_inp_dim=self.state_dim, \n",
    "            state_fc1_dim=64, \n",
    "            action_inp_dim=self.action_dim, \n",
    "            action_fc1_dim=32,\n",
    "            conc_fc1_dim=1024, \n",
    "            conc_fc2_dim=256,\n",
    "            out_dim=1,\n",
    "            lr=self.c_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "        \n",
    "        #Creates the noise generator\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
    "\n",
    "        self.create_plot()\n",
    "        return\n",
    "\n",
    "\n",
    "    def create_plot(self):\n",
    "        self.fig = plt.figure()\n",
    "\n",
    "        self.som_act_plot = self.fig.add_subplot(211)\n",
    "        self.som_act_plot.title.set_text('SOM Activation')\n",
    "\n",
    "        self.instinctive_layer_plot = self.fig.add_subplot(212)\n",
    "        self.instinctive_layer_plot.title.set_text('Instinctive Layer Activations')\n",
    "        return\n",
    "\n",
    "\n",
    "    def update_plots(self, som_act, inst_layer_act):\n",
    "        self.som_act_plot.imshow(som_act)\n",
    "        self.instinctive_layer_plot.imshow(inst_layer_act)\n",
    "\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "        return\n",
    "\n",
    "\n",
    "    def policy(self, state, explore=True):\n",
    "        context, som_act = self.inst_net.get_output(state, self.memory.isMin())\n",
    "        action = self.actor.predict(np.reshape(state, (1, -1)), np.reshape(context, (1, -1)))[0]\n",
    "        #Takes the exploration with the epsilon probability\n",
    "        if explore and np.random.rand() < self.epsilon:\n",
    "            action += self.ou_noise()\n",
    "            \n",
    "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
    "        return action, context, som_act\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, context, next_context, done):\n",
    "        self.memory.append(state, action, reward, next_state, context, next_context, done)\n",
    "        \n",
    "        if self.memory.isMin():\n",
    "            self.replay_memory()\n",
    "        return\n",
    "\n",
    "\n",
    "    def replay_memory(self):\n",
    "        # Get sample experiences from the replay buffer\n",
    "        experiences = self.memory.sample()\n",
    "        \n",
    "        #Get each term of the esxperiences\n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        contexts = np.array([exp[4] for exp in experiences])\n",
    "        next_contexts = np.array([exp[5] for exp in experiences])\n",
    "        done = np.array([int(exp[6]) for exp in experiences])\n",
    "\n",
    "        self.inst_net.train_som(states)\n",
    "        \n",
    "        #Change the dimensions of the rewards and done arrays\n",
    "        rewards = rewards[:, np.newaxis]\n",
    "        done = done[:, np.newaxis]\n",
    "        \n",
    "        #Train the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            #Compute the critic target values\n",
    "            target_actions = self.actor.target_predict(next_states, next_contexts)\n",
    "            y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions) * (1 - done)\n",
    "            #Compute the q_value of each next_state, next_action pair\n",
    "            critic_value = self.critic.predict(states, actions)\n",
    "            #Compute the critic loss \n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
    "        \n",
    "        #Train the actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            acts = self.actor.predict(states, contexts)\n",
    "            critic_grads = self.critic.predict(states, acts)\n",
    "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
    "            \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "        \n",
    "        #Update the model weights\n",
    "        self.actor.transferWeights()\n",
    "        self.critic.transferWeights() \n",
    "        \n",
    "        #Decay the epsilon value\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        #If its reach the minimum value it stops\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def act(self):\n",
    "        #Reset the envirorment\n",
    "        env2 = gym.make(self.env_name, hardcore=True, render_mode='human')\n",
    "        observation, _ = env2.reset()\n",
    "        self.inst_net.reset_charges()\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            env2.render()\n",
    "            action, ctx, som_act = self.policy(observation, explore=False)\n",
    "            #self.update_plots(som_act, self.inst_net.inner_layer.get_activations())\n",
    "            new_observation, ___, done, _, __ = env2.step(action)\n",
    "            observation = new_observation\n",
    "            step += 1\n",
    "            done = done or (step > self.max_steps)\n",
    "        \n",
    "        env2.close()\n",
    "        return\n",
    "\n",
    "\n",
    "    def train(self, env, num_episodes, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch, plot_act):\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "\n",
    "        print(\"BEGIN\\n\")\n",
    "        complete = 0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            steps = 0\n",
    "            observation, _ = env.reset()\n",
    "            self.inst_net.reset_charges()\n",
    "            \n",
    "            while not done:\n",
    "                action, context, som_act = self.policy(observation)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"\\r                                                                                                     \", end=\"\")\n",
    "                    print(f\"\\rEpisode: {str(episode+1)} \\tStep: {str(steps)} \\tReward: {str(score)}\", end=\"\")\n",
    "                \n",
    "                new_observation, reward, done, _, __ = env.step(action)\n",
    "                act, new_context, som_act = self.policy(new_observation)\n",
    "                \n",
    "                if steps > self.max_steps:\n",
    "                    reward = -50\n",
    "                    done = True\n",
    "\n",
    "                self.learn(observation, action, reward, new_observation, context, new_context, done)\n",
    "                observation = new_observation\n",
    "                score += reward\n",
    "                steps += 1\n",
    "\n",
    "                if plot_act:\n",
    "                    self.update_plots(som_act, self.inst_net.inner_layer.get_activations())\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
    "            if(score >= complete_value):\n",
    "                complete += 1\n",
    "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
    "                if end_on_complete and complete >= complete_num: break\n",
    "            \n",
    "            #These information are printed after each verbose_num episodes\n",
    "            if((episode+1)%verbose_num == 0):\n",
    "                print(\"\\r                                                                                                          \", end=\"\")\n",
    "                print(f'''\\rEpisodes: {episode+1}/{num_episodes}\\n\\tTotal reward: {np.mean(scores_history[-verbose_num:])} +- {np.std(scores_history[-verbose_num:])}\\n\\tNum. steps: {np.mean(steps_history[-verbose_num:])} +- {np.std(steps_history[-verbose_num:])}\\n\\tCompleted: {complete}\\n--------------------------''')\n",
    "                \n",
    "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
    "                if act_after_batch: self.act()\n",
    "                \n",
    "                #Set the number of completed episodes on the batch to zero\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        \n",
    "        return scores_history, steps_history\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        self.actor.saveModel(path)\n",
    "        self.critic.saveModel(path)\n",
    "        return\n",
    "\n",
    "\n",
    "    def load(self, a_path, c_path):\n",
    "        self.actor.loadModel(a_path)\n",
    "        self.critic.loadModel(c_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"BipedalWalker-v3\"\n",
    "env = gym.make(name, render_mode='rgb_array', hardcore=True)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "\n",
    "memory_size = 1000000\n",
    "batch_size = 200\n",
    "gamma = 0.99\n",
    "a_lr = 5e-4\n",
    "c_lr = 9e-4\n",
    "tau = 8e-3\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.9999\n",
    "epsilon_min = 0.5\n",
    "max_steps = 1600\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    state_dim, action_dim, action_min, action_max, \n",
    "    memory_size, batch_size, gamma, a_lr, c_lr, tau, \n",
    "    epsilon, epsilon_decay, epsilon_min, max_steps, name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3000\n",
    "verbose = True\n",
    "verbose_num = 50\n",
    "end_on_complete = True\n",
    "complete_num = 2\n",
    "complete_value = 300\n",
    "act_after_batch = True\n",
    "\n",
    "agent.train(\n",
    "    env, num_episodes, verbose, \n",
    "    verbose_num, end_on_complete, \n",
    "    complete_num, complete_value, \n",
    "    act_after_batch, plot_act=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.save('/home/gustavo/PROG/RL_networks/11.1_DDPG_'+name+'/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
